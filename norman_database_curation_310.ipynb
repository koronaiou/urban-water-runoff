{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9061772-7e8c-434b-8bf7-7564362c4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27624c-e296-41c4-b3ce-8aba156db01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "    from rdkit.Chem.SaltRemover import SaltRemover\n",
    "    print(\"âœ… MolStandardize is available\")\n",
    "except Exception as e:\n",
    "    import sys, rdkit\n",
    "    print(\"âŒ MolStandardize not available\")\n",
    "    print(\"RDKit version:\", rdkit.__version__)\n",
    "    print(\"Python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb444dc9-d2d3-40f1-8768-5a4e5c544626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "from rdkit.Chem.SaltRemover import SaltRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d150f-ccf6-471b-ab98-a885885e9301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Load NORMAN CSV ===\n",
    "INPUT_CSV = \"susdat_2025-09-07-201212_filtered.csv\"\n",
    "df = pd.read_csv(INPUT_CSV, dtype=str)   # keep everything as text\n",
    "df = df.dropna(subset=[\"SMILES\"])        # make sure SMILES column exists\n",
    "all_entries = df.to_dict(orient=\"records\")  # list of dicts (each row = entry)\n",
    "\n",
    "# === RDKit processing ===\n",
    "processed = []\n",
    "unique_smiles = set()\n",
    "remover = SaltRemover()\n",
    "uncharger = rdMolStandardize.Uncharger()\n",
    "normalizer = rdMolStandardize.Normalizer()\n",
    "# tautomer_canon = rdMolStandardize.TautomerCanonicalizer()\n",
    "\n",
    "for entry in all_entries:\n",
    "    smiles = entry[\"SMILES\"]\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        # Remove salts/solvents\n",
    "        mol = remover.StripMol(mol, dontRemoveEverything=True)\n",
    "\n",
    "        # Handle mixtures â†’ keep largest fragment by heavy atoms\n",
    "        frags = Chem.GetMolFrags(mol, asMols=True, sanitizeFrags=True)\n",
    "        if len(frags) > 1:\n",
    "            mol = max(frags, key=lambda m: m.GetNumHeavyAtoms())\n",
    "\n",
    "        # Normalize and uncharge\n",
    "        mol = normalizer.normalize(mol)\n",
    "        mol = uncharger.uncharge(mol)\n",
    "        # mol = tautomer_canon.canonicalize(mol)\n",
    "\n",
    "        # Molecular weight filtering\n",
    "        mw = rdMolDescriptors.CalcExactMolWt(mol)\n",
    "        if not (100 <= mw <= 1000):\n",
    "            continue\n",
    "\n",
    "        # Canonical SMILES\n",
    "        can_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "        if can_smiles in unique_smiles:\n",
    "            continue\n",
    "        unique_smiles.add(can_smiles)\n",
    "\n",
    "        # Use existing Formula/ExactMass if present, otherwise compute\n",
    "        if not entry.get(\"Formula\"):\n",
    "            entry[\"Formula\"] = rdMolDescriptors.CalcMolFormula(mol)\n",
    "        if not entry.get(\"ExactMass\"):\n",
    "            entry[\"ExactMass\"] = mw\n",
    "\n",
    "        # Add curated SMILES\n",
    "        entry[\"Canonical_SMILES\"] = can_smiles\n",
    "        processed.append(entry)\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# === Save final output ===\n",
    "cleaned_df = pd.DataFrame(processed)\n",
    "before_dedup = len(cleaned_df)\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=\"Canonical_SMILES\", keep=\"first\")\n",
    "after_dedup = len(cleaned_df)\n",
    "\n",
    "cleaned_df.to_csv(\"Curated_Suspect_List.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Done. Final suspect list contains {after_dedup} unique entries.\")\n",
    "print(f\"ðŸ§¹ Duplicates removed after processing: {before_dedup - after_dedup}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7896002-788a-4be5-b22e-39ea9860d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load curated suspect list ===\n",
    "curated = pd.read_csv(\"Curated_Suspect_List.csv\", dtype=str)\n",
    "\n",
    "def pick(colnames, candidates):\n",
    "    for c in candidates:\n",
    "        if c in colnames: return c\n",
    "    return None\n",
    "\n",
    "def dup_report(frame: pd.DataFrame, label: str):\n",
    "    print(f\"\\n=== Duplicate report: {label} ===\")\n",
    "    cols = frame.columns\n",
    "\n",
    "    name_col   = pick(cols, [\"Name\",\"Substance\",\"CompoundName\",\"Compound\"])\n",
    "    smiles_col = pick(cols, [\"SMILES\",\"Canonical_SMILES\",\"Smiles\"])\n",
    "    can_smi    = \"Canonical_SMILES\" if \"Canonical_SMILES\" in cols else None\n",
    "    inchikey   = pick(cols, [\"StdInChIKey\",\"InChIKey\",\"InchiKey\",\"STDINCHIKEY\"])\n",
    "    inchi      = pick(cols, [\"InChI\",\"StdInChI\",\"INCHI\",\"STDINCHI\"])\n",
    "\n",
    "    # InChIKey first block (IK14)\n",
    "    ik14 = None\n",
    "    if inchikey:\n",
    "        ik14 = frame[inchikey].astype(str).str.split(\"-\").str[0]\n",
    "\n",
    "    # normalized name for duplicate check\n",
    "    name_norm = frame[name_col].astype(str).str.strip().str.lower() if name_col else None\n",
    "\n",
    "    def count_dups(series, desc):\n",
    "        s = series.dropna().astype(str)\n",
    "        n_total = len(series)\n",
    "        n_nonempty = s.ne(\"\").sum()\n",
    "        n_unique = s.replace(\"\", pd.NA).dropna().nunique()\n",
    "        n_dups = n_nonempty - n_unique\n",
    "        print(f\"{desc:>20}: total={n_total}, non-empty={n_nonempty}, unique={n_unique}, duplicates={n_dups}\")\n",
    "\n",
    "        # show top repeated values\n",
    "        if n_dups > 0:\n",
    "            vc = s[s.ne(\"\")].value_counts()\n",
    "            top = vc[vc>1].head(10)\n",
    "            if not top.empty:\n",
    "                print(f\"Top duplicates for {desc}:\")\n",
    "                display(pd.DataFrame({\"count\": top}))\n",
    "\n",
    "    if name_col:\n",
    "        count_dups(frame[name_col], \"Name (raw)\")\n",
    "        count_dups(name_norm,       \"Name (normalized)\")\n",
    "    if smiles_col:\n",
    "        count_dups(frame[smiles_col], \"SMILES\")\n",
    "    if can_smi and can_smi != smiles_col:\n",
    "        count_dups(frame[can_smi], \"Canonical_SMILES\")\n",
    "    if inchikey:\n",
    "        count_dups(frame[inchikey], \"InChIKey (full)\")\n",
    "    if ik14 is not None:\n",
    "        count_dups(ik14, \"InChIKey IK14\")\n",
    "    if inchi:\n",
    "        count_dups(frame[inchi], \"InChI (full)\")\n",
    "\n",
    "# Run on curated file\n",
    "dup_report(curated, \"Curated_Suspect_List.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py310-env]",
   "language": "python",
   "name": "conda-env-.conda-py310-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
